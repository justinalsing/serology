{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Implementaton of Laplace Approximation for GP Hyperparameters (w/ Internal Newton Optimiser)\n",
    "This is an example python script to illustrate LA GP fitting with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in summarised TRaC data with matched covariates and build as TensorFlow constants \n",
    "\n",
    "- Age restricted to 1-50 y/o (to limit impact of maternal anti-bodies and limit look-back time)\n",
    "\n",
    "- Sites snapped to land-sea mask boundary and standardised covariates extracted\n",
    "\n",
    "- Area including Port-au-Prince and surrounds excluded (to limit impact of migration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.io import mmread\n",
    "import osgeo.gdal as gdal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Start a tensorflow session\n",
    "sess = tf.Session(config = tf.ConfigProto())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Import the trac data\n",
    "tracdata = pd.read_csv('../trac_data/summary_TRaC_data_with_covariates.csv')\n",
    "\n",
    "# Village positions\n",
    "village_lats = tracdata.groupby(['Cluster_Num'])['Latitude'].unique().values\n",
    "village_longs = tracdata.groupby(['Cluster_Num'])['Longitude'].unique().values\n",
    "\n",
    "# Number of villages\n",
    "Nvillages = tracdata.Cluster_Num.unique().size\n",
    "\n",
    "# Extract the titres and ages for all individuals, grouped by village\n",
    "titres = tf.constant(np.log(np.abs(tracdata.AMA.values + 30)), dtype='float32')\n",
    "ages = tf.constant(tracdata.Age.values, dtype='float32')\n",
    "cluster_assignments = tf.constant(tracdata.Cluster_Num.values, dtype='int32')\n",
    "\n",
    "# Number of individuals\n",
    "Nindividuals = len(tracdata.Age.values)\n",
    "\n",
    "# Extract the mean covariates in each cell\n",
    "covariates = tracdata.groupby(['Cluster_Num'])['covariate_accessibility','covariate_AI','covariate_distTowater','covariate_elevation','covariate_forest','covariate_grass','covariate_urbanbarren','covariate_woodysavanna','covariate_OSM','covariate_PET','covariate_slope','covariate_TWI'].mean().values\n",
    "covariates = tf.constant(covariates, dtype='float32')\n",
    "\n",
    "# Number of covariates\n",
    "Ncovariates = covariates.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the batch normalizations for re-scaling inputs to the neural likelihood\n",
    "f = open('simulations/simulations/model_2/batch_normalizations.pkl', 'rb')\n",
    "xs_mean, xs_std, ps_mean, ps_std = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Data = individual titres re-scaled by batch norms for input to the neural likelihood\n",
    "data_individual = (tf.expand_dims(titres,1)-xs_mean)/ xs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in matrices for INLA mesh projections\n",
    "\n",
    "# Projection from INLA grid to cluster locations\n",
    "Amatrix = mmread('data/A_matrix.mtx')\n",
    "\n",
    "# Size of INLA grid\n",
    "Nmesh = Amatrix.shape[1]\n",
    "\n",
    "# The various bits of the SPDE precision matrix (on the INLA grid)\n",
    "M0 = mmread('data/M0_matrix.mtx')\n",
    "M1 = mmread('data/M1_matrix.mtx')\n",
    "M2 = mmread('data/M2_matrix.mtx')\n",
    "\n",
    "# Cast to tf constants\n",
    "Amatrix = tf.constant(Amatrix.toarray(), dtype='float32') # Not sure if it's worth figuring out how to use SparseTensors\n",
    "M0 = tf.constant(M0.toarray(), dtype='float32')\n",
    "M1 = tf.constant(M1.toarray(), dtype='float32')\n",
    "M2 = tf.constant(M2.toarray(), dtype='float32')\n",
    "\n",
    "# Matrix to project INLA triangular grid to the full resolution of the covariates\n",
    "Afullmatrix = mmread('data/Afull_matrix.mtx')\n",
    "Afullmatrix = Afullmatrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP hyper-parameter kappa\n",
    "log_kappa = tf.Variable(1.0, dtype ='float32')\n",
    "prior_log_kappa = tfp.distributions.Normal(loc = 0.0, scale = 1.0)\n",
    "log_tau = tf.Variable(-3.0, dtype = 'float32')\n",
    "prior_log_tau = tfp.distributions.Normal(loc = 0.0, scale = 1.0)\n",
    "\n",
    "# Set-up GP for the log-field\n",
    "log_field = tf.constant(np.zeros(Nmesh), dtype='float32')\n",
    "spde_prec = (tf.exp(log_kappa*4.0)*M0 + tf.constant(2.0, dtype ='float32')*tf.exp(log_kappa*2.0)*M1 + M2)*tf.exp(log_tau*tf.constant(2.0,dtype = 'float32'))\n",
    "spde_cov = tf.matrix_inverse(spde_prec)\n",
    "prior_log_field = tfp.distributions.MultivariateNormalFullCovariance(loc = tf.constant(np.zeros(Nmesh), dtype='float32'), covariance_matrix = spde_cov)\n",
    "\n",
    "# Slopes and intercept for linear predictor part of the model\n",
    "slopes = tf.Variable(np.zeros(Ncovariates), dtype='float32')\n",
    "prior_slopes = tfp.distributions.Normal(loc = 0.0, scale = 1.0)\n",
    "intercept = tf.Variable(0.0, dtype='float32')\n",
    "prior_intercept = tfp.distributions.Normal(loc = 0.0, scale = 5.0)\n",
    "\n",
    "# Serology hyper-parameters\n",
    "serology = tf.Variable(np.array([\n",
    "                          np.log(0.5), # logHetExp\n",
    "                          0.25, # fzero\n",
    "                          20, # azero\n",
    "                          np.log(0.5), # logHetBoosting\n",
    "                          9, # baselineBoostingFactor\n",
    "                          np.log(8), # logBaseineBoostingThreshold\n",
    "                          np.log(1), # logAttenuationFactor\n",
    "                          12,# ageAdultAntibodies\n",
    "                          np.log(1), # logRhoChild\n",
    "                          np.log(3), # logRhoAdult\n",
    "                          5, # mean background antibody levels\n",
    "                          np.log(10), # log std dev antibody background level\n",
    "                          ]), dtype='float32')\n",
    "\n",
    "# Serology prior\n",
    "serology_lower = tf.constant(np.array([\n",
    "                          np.log(0.05), # logHetExp\n",
    "                          0, # fzero\n",
    "                          10, # azero\n",
    "                          np.log(0.05), # logHetBoosting\n",
    "                          6, # baselineBoostingFactor\n",
    "                          np.log(4), # logBaseineBoostingThreshold\n",
    "                          np.log(0.05), # logAttenuationFactor\n",
    "                          5,# ageAdultAntibodies\n",
    "                          np.log(0.1), # logRhoChild\n",
    "                          np.log(0.1), # logRhoAdult\n",
    "                          -5, # mean background antibody levels\n",
    "                          np.log(1), # log std dev antibody background level\n",
    "                          ]), dtype='float32')\n",
    "\n",
    "serology_upper = tf.constant(np.array([\n",
    "                          np.log(1), # logHetExp\n",
    "                          0.5, # fzero\n",
    "                          30, # azero\n",
    "                          np.log(1), # logHetBoosting\n",
    "                          12, # baselineBoostingFactor\n",
    "                          np.log(12), # logBaseineBoostingThreshold\n",
    "                          np.log(2), # logAttenuationFactor\n",
    "                          20,# ageAdultAntibodies\n",
    "                          np.log(5), # logRhoChild\n",
    "                          np.log(40), # logRhoAdult ### log(40) for MSP and AMA, log(10) for LSA ###\n",
    "                          15, # mean background antibody levels\n",
    "                          np.log(20), # log std dev antibody background level\n",
    "                          ]), dtype='float32')\n",
    "\n",
    "prior_serology = tfp.distributions.Uniform(low=serology_lower, high=serology_upper)\n",
    "\n",
    "# Additional prior on the maximum infection rate: only trained the NDE upto a certain value\n",
    "prior_linear_predictor = tfp.distributions.Uniform(low=tf.constant(-1e10, dtype='float32'), high=tf.constant(np.log(10), dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up for the neural density estimator:\n",
    "# read in weights and biases, set the architecture (to same as the trained NDE)\n",
    "\n",
    "# Load in the network weights and biases\n",
    "f = open('simulations/simulations/model_2/network_weights.pkl', 'rb')\n",
    "network_weights = pickle.load(f)\n",
    "f.close()\n",
    "f = open('simulations/simulations/model_2/network_biases.pkl', 'rb')\n",
    "network_biases = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Declare weights and biases as tf.constants\n",
    "weights = [tf.constant(network_weights[0], dtype='float32'), tf.constant(network_weights[1], dtype='float32')]\n",
    "biases = [tf.constant(network_biases[0], dtype='float32'), tf.constant(network_biases[1], dtype='float32')]\n",
    "\n",
    "# (Global) set-up parameters for the NDE\n",
    "n_antibodies = 1 # number of antibodies\n",
    "n_parameters = 12 + 1 # serology + log_eir\n",
    "D = n_antibodies # size of data (number of antibodies)\n",
    "P = n_parameters + 1 # number of parameters (num serology parameters + EIR + age)\n",
    "M = 10 # number of Gaussian components\n",
    "N = int((D + D * (D + 1) / 2 + 1)*M)\n",
    "n_hidden = [100,100]\n",
    "activations = [tf.tanh, tf.tanh]\n",
    "\n",
    "# Functions for mapping the neural network output to the mixture model components\n",
    "\n",
    "# Build lower triangular matrices (parameterizing the covariances) from network output (also calculate determinant)\n",
    "def lower_triangular_matrix(σ):\n",
    "    Σ = []\n",
    "    det = []\n",
    "    start = 0\n",
    "    end = 1\n",
    "    for i in range(D):\n",
    "        exp_val = tf.exp(σ[:, :, end-1])\n",
    "        det.append(exp_val)\n",
    "        if i > 0:\n",
    "            Σ.append(tf.pad(tf.concat([σ[:, :, start:end-1], tf.expand_dims(exp_val, -1)], -1), [[0, 0], [0, 0], [0, D-i-1]]))\n",
    "        else:\n",
    "            Σ.append(tf.pad(tf.expand_dims(exp_val, -1), [[0, 0], [0, 0], [0, D-i-1]]))\n",
    "        start = end\n",
    "        end += i + 2\n",
    "    Σ = tf.transpose(tf.stack(Σ), (1, 2, 0, 3))\n",
    "    det = tf.reduce_prod(tf.stack(det), 0)\n",
    "    return Σ, det\n",
    "\n",
    "# Split network output into component means, covariances and weights (also returns determinant of covariance)\n",
    "def mapping(output_layer, M, D):\n",
    "    μ, Σ, α = tf.split(output_layer, [M * D, M * D * (D + 1) // 2, M], 1)\n",
    "    μ = tf.reshape(μ, (-1, M, D))\n",
    "    Σ, det = lower_triangular_matrix(tf.reshape(Σ, (-1, M, D * (D + 1) // 2)))\n",
    "    α = tf.nn.softmax(α)\n",
    "    return μ, Σ, α, det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NOTE: Cell below constructs log posterior for use outside the Newton optimization while_loop()\n",
    "##### Skip this if you want everything done inside the Newton optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct log posterior here for use outside the Newton optimization while_loop()\n",
    "\n",
    "# Convert log field to tensor\n",
    "log_field = tf.convert_to_tensor(log_field)\n",
    "\n",
    "# Linear predictor model for EIR: GP + C\\beta + \\beta_0\n",
    "linear_predictor = tf.squeeze(tf.matmul(Amatrix,tf.expand_dims(log_field,1))) + intercept + tf.squeeze(tf.matmul(covariates,tf.expand_dims(slopes,1)))\n",
    "\n",
    "# Set up the parameters vector: list of (lambda_EIR, serology_parameters, age) for each individual\n",
    "# NOTE: neural likelihood currently trained on EIR, NOT log(EIR), hence the exp() -- this will change on the next run\n",
    "theta = (tf.concat([tf.expand_dims(tf.gather(linear_predictor, cluster_assignments-1),1), # EIR at the individuals locations\n",
    "                                tf.transpose(tf.tile(tf.reshape(serology, [-1,1]), [1,Nindividuals])), # Shared serology parameters for all individuals\n",
    "                                tf.expand_dims(ages,1)], axis=1) - ps_mean)/ ps_std # Ages, and then shift and re-scale everything according to the batch normalization for the neural likelihood\n",
    "\n",
    "# Log prior over hyper-parameters and log-field (GP)\n",
    "log_prior = prior_log_kappa.log_prob(log_kappa) \\\n",
    "+ prior_log_tau.log_prob(log_tau) \\\n",
    "+ prior_log_field.log_prob(log_field) \\\n",
    "+ tf.reduce_sum(prior_slopes.log_prob(slopes)) \\\n",
    "+ prior_intercept.log_prob(intercept) \\\n",
    "+ tf.reduce_sum(prior_serology.log_prob(serology)) \\\n",
    "+ tf.reduce_sum(prior_linear_predictor.log_prob(linear_predictor)) # extra prior over the linear predictor to make sure it stays within the range of the trained NDE\n",
    "\n",
    "# Set up the NDE likelihood\n",
    "\n",
    "# Build the layers of the network\n",
    "\n",
    "# First layer (input) is just the parameters\n",
    "layers = [theta] \n",
    "\n",
    "# Now loop through the hidden layers\n",
    "for i in range(len(n_hidden)):\n",
    "    if i < len(n_hidden) - 1:\n",
    "        layers.append(activations[i](tf.add(tf.matmul(layers[i], weights[i]), biases[i])))\n",
    "    else:\n",
    "        layers.append(tf.add(tf.matmul(layers[i], weights[i]), biases[i]))\n",
    "\n",
    "# Map the output layer to mixture model parameters\n",
    "μ, Σ, α, det = mapping(layers[-1], M, D)\n",
    "\n",
    "# Compute the log likelihood\n",
    "log_likelihood = tf.reduce_sum(tf.log(tf.reduce_sum(tf.exp(-0.5*tf.reduce_sum(tf.square(tf.einsum(\"ijlk,ijk->ijl\", Σ, tf.subtract(tf.expand_dims(data_individual, 1), μ))), 2) + tf.log(α) + tf.log(det) - D*np.log(2. * np.pi) / 2.), 1) + 1e-37))\n",
    "\n",
    "# Log prior over hyper-parameters and log-field (GP)\n",
    "log_prior = prior_log_kappa.log_prob(log_kappa) \\\n",
    "+ prior_log_tau.log_prob(log_tau) \\\n",
    "+ prior_log_field.log_prob(log_field) \\\n",
    "+ tf.reduce_sum(prior_slopes.log_prob(slopes)) \\\n",
    "+ prior_intercept.log_prob(intercept) \\\n",
    "+ tf.reduce_sum(prior_serology.log_prob(serology)) \\\n",
    "+ tf.reduce_sum(prior_linear_predictor.log_prob(linear_predictor)) # extra prior over the linear predictor to make sure it stays within the range of the trained NDE\n",
    "\n",
    "# Negative log posterior\n",
    "negative_log_posterior_prob = -(log_likelihood + log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16447.28\n",
      "[array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.96323578e+02,\n",
      "       1.47083069e+02, 1.00472153e+02, 8.41484833e+01, 8.90565186e+01,\n",
      "       1.34108612e+02, 2.22411530e+02, 1.42249756e+02, 7.96238174e+01,\n",
      "       8.05265198e+01, 6.81661835e+01, 6.28338089e+01, 1.06631355e+02,\n",
      "       1.30969269e+02, 1.38040649e+02, 1.30948746e+02, 2.13318710e+02,\n",
      "       1.15823006e+02, 6.13648415e+01, 1.25907700e+02, 2.10615997e+02,\n",
      "       1.44091644e+02, 8.28289032e+01, 1.04463310e+02, 1.41772873e+02,\n",
      "       1.39215668e+02, 2.36668564e+02, 1.69726013e+02, 1.50596848e+02,\n",
      "       1.21433571e+02, 1.68936783e+02, 1.19200279e+02, 1.58604141e+02,\n",
      "       9.04041901e+01, 2.70858631e+01, 4.50477982e+01, 4.13248329e+01,\n",
      "       1.21895935e+02, 1.61065781e+02, 1.39435364e+02, 1.23567894e+02,\n",
      "       1.06102394e+02, 1.34185898e+02, 3.10581757e+02, 1.85590927e+02,\n",
      "       1.18260880e+02, 1.24606735e+02, 1.07097816e+02, 9.28756943e+01,\n",
      "       1.37843369e+02, 6.01448593e+01, 4.53593369e+01, 7.20702515e+01,\n",
      "       2.97375393e+01, 5.55193176e+01, 4.53657036e+01, 4.78800697e+01,\n",
      "       3.61932144e+01, 7.86279373e+01, 5.53134155e+01, 3.91180000e+01,\n",
      "       7.73164825e+01, 4.29105911e+01, 8.44144821e+01, 6.02531471e+01,\n",
      "       6.88829803e+01, 6.51971436e+01, 7.15745773e+01, 2.74766846e+01,\n",
      "       3.52938499e+01, 4.78391228e+01, 3.60559692e+01, 4.61793709e+01,\n",
      "       4.82644005e+01, 5.04580193e+01, 5.45783615e+01, 4.90167694e+01,\n",
      "       3.76666298e+01, 3.46348343e+01, 6.16552238e+01, 5.28205566e+01,\n",
      "       8.13206100e+01, 3.93206825e+01, 4.70871735e+01, 4.13193016e+01,\n",
      "       7.46510391e+01, 4.14471741e+01, 5.74599457e+01, 6.38937187e+01,\n",
      "       5.78588333e+01, 5.75375748e+01, 5.56630478e+01, 3.90456123e+01,\n",
      "       3.60181770e+01, 8.27815247e+01, 5.94571877e+01, 5.41768608e+01,\n",
      "       4.31376572e+01, 4.47388420e+01, 4.87012215e+01, 5.95387726e+01,\n",
      "       4.97922630e+01, 6.58798599e+01, 6.25950584e+01, 6.10306168e+01,\n",
      "       4.19058113e+01, 9.28972931e+01, 5.08121872e+01, 8.31178284e+01,\n",
      "       9.23587952e+01, 5.98263359e+01, 6.60406036e+01, 4.31258049e+01,\n",
      "       4.87492294e+01, 9.62474518e+01, 2.81220798e+01, 5.40567513e+01,\n",
      "       2.93533096e+01, 8.52891998e+01, 6.38004570e+01, 5.94220734e+01,\n",
      "       4.50583839e+01, 3.49880867e+01, 5.54283791e+01, 4.80791054e+01,\n",
      "       5.18563995e+01, 4.79040031e+01, 5.23682442e+01, 2.39954166e+01,\n",
      "       3.82366447e+01, 5.85682526e+01, 5.22179146e+01, 2.38843079e+01,\n",
      "       2.91211395e+01, 7.29925537e+01, 4.27374268e+01, 4.95938454e+01,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 3.55261040e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 8.12310800e-02, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 1.04822168e+01, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 4.98479557e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 9.56190109e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 2.33678684e+01, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00], dtype=float32)]\n",
      "[array([[ 6.93409741e-01,  1.11741500e-07,  1.68307153e-07, ...,\n",
      "        -5.58337589e-08, -2.47779310e-01, -7.16277191e-06],\n",
      "       [ 1.12819905e-07,  8.76947105e-01,  1.73618384e-02, ...,\n",
      "        -2.78356075e-01,  6.47452154e-08, -1.48491168e-07],\n",
      "       [ 1.68652207e-07,  1.73618700e-02,  1.34657311e+00, ...,\n",
      "        -1.11232896e-06,  2.49787107e-08, -3.65700465e-08],\n",
      "       ...,\n",
      "       [-5.42404202e-08, -2.78356016e-01, -1.10880671e-06, ...,\n",
      "         7.35344231e-01, -5.39001057e-08,  1.19409805e-07],\n",
      "       [-2.47779340e-01,  6.60678197e-08,  2.49727528e-08, ...,\n",
      "        -5.39001093e-08,  7.03203559e-01,  5.80923461e-06],\n",
      "       [-7.19575519e-06, -1.48486421e-07, -3.65604507e-08, ...,\n",
      "         1.19409819e-07,  5.80923415e-06,  9.03659105e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Test the gradients and hessian for the log posterior\n",
    "# NOTE: getting a lot of zeros in the gradients and hessian, expected?\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(negative_log_posterior_prob))\n",
    "print(sess.run(tf.gradients(negative_log_posterior_prob, log_field)))\n",
    "print(sess.run(tf.hessians(negative_log_posterior_prob, log_field)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton optimization iteration to optimize p(z | \\theta, x) for fixed \\theta and data x and find laplace approximation for it\n",
    "def inner_vector_update(log_field, iter_diff, log_laplace_approximation, output_collections=(), name=None):\n",
    "    \n",
    "    # Convert log field to tensor\n",
    "    log_field = tf.convert_to_tensor(log_field)\n",
    "    \n",
    "    # Linear predictor model for EIR: GP + C\\beta + \\beta_0\n",
    "    linear_predictor = tf.squeeze(tf.matmul(Amatrix,tf.expand_dims(log_field,1))) + intercept + tf.squeeze(tf.matmul(covariates,tf.expand_dims(slopes,1)))\n",
    "\n",
    "    # Set up the parameters vector: list of (lambda_EIR, serology_parameters, age) for each individual\n",
    "    theta = (tf.concat([tf.expand_dims(tf.gather(linear_predictor, cluster_assignments-1),1), \n",
    "                                    tf.transpose(tf.tile(tf.reshape(serology, [-1,1]), [1,Nindividuals])), \n",
    "                                    tf.expand_dims(ages,1)], axis=1) - ps_mean)/ ps_std\n",
    "    \n",
    "    # Log prior over hyper-parameters and log-field (GP)\n",
    "    log_prior = prior_log_kappa.log_prob(log_kappa) \\\n",
    "    + prior_log_tau.log_prob(log_tau) \\\n",
    "    + prior_log_field.log_prob(log_field) \\\n",
    "    + tf.reduce_sum(prior_slopes.log_prob(slopes)) \\\n",
    "    + prior_intercept.log_prob(intercept) \\\n",
    "    + tf.reduce_sum(prior_serology.log_prob(serology)) \\\n",
    "    + tf.reduce_sum(prior_linear_predictor.log_prob(linear_predictor)) # extra prior over the linear predictor to make sure it stays within the range of the trained NDE\n",
    "    \n",
    "    # Set up the NDE likelihood\n",
    "\n",
    "    # Build the layers of the network\n",
    "    \n",
    "    # First layer (input) is just the parameters\n",
    "    layers = [theta] \n",
    "\n",
    "    # Now loop through the hidden layers\n",
    "    for i in range(len(n_hidden)):\n",
    "        if i < len(n_hidden) - 1:\n",
    "            layers.append(activations[i](tf.add(tf.matmul(layers[i], weights[i]), biases[i])))\n",
    "        else:\n",
    "            layers.append(tf.add(tf.matmul(layers[i], weights[i]), biases[i]))\n",
    "\n",
    "    # Map the output layer to mixture model parameters\n",
    "    μ, Σ, α, det = mapping(layers[-1], M, D)\n",
    "\n",
    "    # Compute the log likelihood\n",
    "    log_likelihood = tf.reduce_sum(tf.log(tf.reduce_sum(tf.exp(-0.5*tf.reduce_sum(tf.square(tf.einsum(\"ijlk,ijk->ijl\", Σ, tf.subtract(tf.expand_dims(data_individual, 1), μ))), 2) + tf.log(α) + tf.log(det) - D*np.log(2. * np.pi) / 2.), 1) + 1e-37))\n",
    "    \n",
    "    # Negative log posterior\n",
    "    negative_log_posterior_prob = -(log_likelihood + log_prior)\n",
    "\n",
    "    # Do a Newton step towards the optimal log-field and record difference\n",
    "    new_log_field = log_field - tf.constant(0.5, dtype = 'float32')*tf.squeeze(tf.matmul(tf.squeeze(tf.matrix_inverse(tf.hessians(negative_log_posterior_prob,log_field))),tf.expand_dims(tf.squeeze(tf.gradients(negative_log_posterior_prob,log_field)),1)))\n",
    "    new_iter_diff = tf.reduce_sum(tf.abs(log_field-new_log_field))\n",
    "  \n",
    "    # New laplace approximation for p(z | \\theta, x)\n",
    "    new_log_laplace_approximation = tf.constant(Nmesh*0.9189385, dtype = 'float32') - tf.constant(0.5, dtype = 'float32')*tf.linalg.logdet(tf.squeeze(tf.hessians(negative_log_posterior_prob,log_field))) - negative_log_posterior_prob\n",
    "        \n",
    "    return [new_log_field, new_iter_diff, new_log_laplace_approximation]\n",
    "\n",
    "# Convergence criterion for the Newton optimizer\n",
    "def running_condition(log_field,iter_diff,log_laplace_approximation, output_collections=(), name=None):\n",
    "    return tf.greater(iter_diff,tf.constant(0.001, dtype = 'float32')) \n",
    "\n",
    "# Initialize log laplace approximation and convergence criterion test\n",
    "log_laplace_approximation = tf.constant(0.0, dtype = 'float32')\n",
    "iter_diff = tf.constant(100000000000000000000.0, dtype = 'float32')\n",
    "\n",
    "# Do the Newton optimization for fixed \\theta\n",
    "iteration_outputs = tf.while_loop(running_condition,inner_vector_update,[log_field,iter_diff,log_laplace_approximation])\n",
    "\n",
    "# log laplace approximation for p(z | \\theta, x) evaluated at the mode z_*, and corresponding mode z_*\n",
    "log_laplace_approx_at_conditional_mode = iteration_outputs[2]\n",
    "log_field_at_conditional_mode = iteration_outputs[0]\n",
    "\n",
    "# Set up the optimizer to find MAP hyper parameters\n",
    "optimiser = tf.train.AdagradOptimizer(0.05) # Behaviour sensitive to this training rate!\n",
    "train = optimiser.minimize(-log_laplace_approx_at_conditional_mode, var_list = [log_kappa, log_tau, slopes, intercept, serology])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.6640753 , -2.5601163 , -2.7671826 , -2.274752  , -2.5726936 ,\n",
       "        -1.9312786 , -1.4574317 , -1.2376482 , -3.4472866 , -2.1735246 ,\n",
       "        -1.449919  , -1.4850017 , -1.8750013 , -2.66414   , -1.6471913 ,\n",
       "        -1.2908758 , -1.7787113 , -2.4605691 , -1.9019362 , -2.0773687 ,\n",
       "        -1.8229811 , -2.7437313 , -2.7499735 , -3.142415  , -1.90661   ,\n",
       "        -2.9646356 , -1.9925734 , -1.1971501 , -2.0795033 , -2.3252869 ,\n",
       "        -3.3381054 , -1.9036347 , -2.8692381 , -2.1092405 , -2.9932594 ,\n",
       "        -2.7513368 , -1.9382454 , -1.5481589 , -2.017346  , -2.671078  ,\n",
       "        -2.3185863 , -2.249017  , -2.3866315 , -3.266153  , -4.277514  ,\n",
       "        -3.3124268 , -2.618866  , -2.4900115 , -3.3182497 , -3.7168539 ,\n",
       "        -3.7172313 , -2.2178054 , -2.1185226 , -1.5296483 , -2.570377  ,\n",
       "        -2.80674   , -3.0931425 , -2.8638387 , -2.7282252 , -3.219773  ,\n",
       "        -2.92779   , -1.6430484 , -4.239279  , -3.870586  , -4.003503  ,\n",
       "        -3.8188682 , -3.3678331 , -3.6018255 , -3.7214065 , -3.4168863 ,\n",
       "        -2.7382932 , -2.4219265 , -2.1634662 , -3.6974518 , -3.7931457 ,\n",
       "        -2.902512  , -3.1019754 , -2.7926054 , -2.6166844 , -2.3200967 ,\n",
       "        -2.763675  , -3.3053098 , -4.343927  , -1.9085495 , -1.8479109 ,\n",
       "        -2.272497  , -2.9851072 , -2.749649  , -1.8664002 , -1.8310636 ,\n",
       "        -1.7604774 , -2.5738413 , -2.197721  , -3.3300366 , -2.610413  ,\n",
       "        -3.0159504 , -1.9047242 , -3.142382  , -4.14743   , -2.977668  ,\n",
       "        -3.3811038 , -4.190576  , -3.7692928 , -3.3447607 , -2.6466773 ,\n",
       "        -2.2499588 , -2.8379762 , -3.610467  , -3.6953592 , -4.219775  ,\n",
       "        -3.2580538 , -1.8171124 , -2.011072  , -2.8889356 , -2.097368  ,\n",
       "        -1.7861568 , -2.3933659 , -1.7259375 , -2.9776602 , -2.2691305 ,\n",
       "        -2.633378  , -2.4545972 , -2.9192195 , -2.9299    , -3.9402013 ,\n",
       "        -2.9707448 , -2.661345  , -1.9524307 , -2.7501843 , -4.048552  ,\n",
       "        -3.1228502 , -3.7032888 , -2.4157343 , -3.5979347 , -2.7309084 ,\n",
       "        -2.8742862 , -2.5377507 , -3.597583  , -4.5243416 , -2.6835158 ,\n",
       "        -1.8516722 , -1.9318799 , -2.5066197 , -3.1381783 , -3.1475925 ,\n",
       "        -2.7729764 , -3.3199627 , -2.2743917 , -2.559909  , -3.3503776 ,\n",
       "        -2.5390882 , -3.2227323 , -2.9837806 , -2.5047035 , -2.1296198 ,\n",
       "        -2.7982378 , -2.5857804 , -3.8599176 , -3.0111563 , -3.5747454 ,\n",
       "        -2.9919648 , -3.8146074 , -3.1103694 , -3.198081  , -2.6764643 ,\n",
       "        -2.4260597 , -2.1052015 , -3.0468593 , -2.9564536 , -2.8623803 ,\n",
       "        -2.6331277 , -1.749193  , -1.5760932 , -3.4815001 , -2.9697409 ,\n",
       "        -2.0288734 , -2.1404145 , -3.237404  , -2.90623   , -1.9098747 ,\n",
       "        -2.7373257 , -2.8177097 , -3.0100734 , -3.6305356 , -3.1293836 ,\n",
       "        -2.5683634 , -3.145512  , -3.1139565 , -2.6379726 , -3.444906  ,\n",
       "        -3.52849   , -2.4012725 , -2.9469478 , -3.1491082 , -3.13673   ,\n",
       "        -2.7063687 , -2.1915064 , -1.8665643 , -2.8003857 , -2.5542588 ,\n",
       "        -2.8556213 , -2.8668578 , -3.588486  , -2.7258248 , -3.4613612 ,\n",
       "        -2.88461   , -2.52148   , -2.7542706 , -2.4369411 , -2.7091982 ,\n",
       "        -2.7450926 , -2.4784813 , -2.822793  , -3.5828302 , -3.5940778 ,\n",
       "        -3.5681858 , -3.1157026 , -2.130751  , -2.2804742 , -3.2886262 ,\n",
       "        -2.761386  , -3.483449  , -3.3066006 , -2.301657  , -3.0414824 ,\n",
       "        -4.2256007 , -1.8925177 , -1.6806265 , -3.7425613 , -1.8890655 ,\n",
       "        -2.489599  , -3.4764214 , -2.8836393 , -2.7928414 , -3.4966524 ,\n",
       "        -3.4493392 , -3.5327861 , -3.4093812 , -3.1807876 , -1.6174097 ,\n",
       "        -2.6172197 , -2.0587833 , -2.617744  , -1.7695224 , -3.4617715 ,\n",
       "        -2.3592892 , -3.3177636 , -2.4816709 , -1.8370012 , -2.6959674 ,\n",
       "        -2.8525298 , -3.1515    , -3.9228997 , -3.532465  , -2.3036187 ,\n",
       "        -3.091066  , -2.8298578 , -2.2965143 , -3.5103338 , -2.900672  ,\n",
       "        -3.2931042 , -2.310761  , -2.5838912 , -2.0868223 , -2.8409672 ,\n",
       "        -2.5706153 , -2.6154811 , -2.5212646 , -3.4102812 , -2.4298441 ,\n",
       "        -3.0726786 , -1.6498204 , -2.438201  , -3.9017043 , -4.062252  ,\n",
       "        -3.2134562 , -2.387052  , -2.5033655 , -2.723625  , -2.7653673 ,\n",
       "        -3.1015582 , -2.109035  , -1.9879373 , -2.23044   , -2.0621753 ,\n",
       "        -2.80141   , -2.8523355 , -3.7169764 , -3.2849205 , -2.9002857 ,\n",
       "        -3.7597382 , -3.562309  , -3.3670201 , -3.6020532 , -2.7914302 ,\n",
       "        -2.7595546 , -3.0616975 , -2.9788246 , -3.716225  , -4.1339374 ,\n",
       "        -2.7726212 , -3.205573  , -3.1581237 , -2.4881175 , -3.4277732 ,\n",
       "        -2.880375  , -3.4986672 , -3.3157368 , -2.6009598 , -2.4532945 ,\n",
       "        -2.9849505 , -2.7302148 , -2.1045334 , -1.9466985 , -1.7295619 ,\n",
       "        -1.8830094 , -2.2579544 , -3.5602264 , -3.685576  , -3.264025  ,\n",
       "        -3.1201847 , -3.2389605 , -2.6606877 , -2.8848698 , -2.6157188 ,\n",
       "        -2.6763003 , -2.7719755 , -2.32757   , -2.4538326 , -2.2673044 ,\n",
       "        -1.8370105 , -1.914203  , -1.0653895 , -1.9029814 , -1.9232975 ,\n",
       "        -2.0068362 , -1.9971135 , -1.4670185 , -1.6590618 , -1.8482238 ,\n",
       "        -1.7649003 , -1.2640718 , -1.2648238 , -1.043767  , -0.90574294,\n",
       "        -0.7080239 , -0.70759755, -0.9539572 , -2.3549228 , -1.1589175 ,\n",
       "        -1.5967289 , -1.2231567 , -1.4115112 , -2.241667  , -1.3026214 ,\n",
       "        -1.2396266 , -1.3656955 , -1.712555  , -1.2804577 , -2.2323513 ,\n",
       "        -2.187514  , -1.4604701 , -1.2686309 , -1.4547855 , -1.940832  ,\n",
       "        -0.8065635 , -1.4118292 , -1.8707993 , -1.3008754 , -2.1933544 ,\n",
       "        -0.8139123 , -1.4453422 , -2.159086  , -2.1823974 , -1.77338   ,\n",
       "        -2.0232441 , -1.5833509 , -1.5212744 , -2.519009  , -1.2398677 ,\n",
       "        -1.983043  , -1.1941392 , -2.0680237 , -0.8165264 , -1.2176834 ,\n",
       "        -2.4390242 , -1.875005  , -1.0176026 , -1.97529   , -1.1695931 ,\n",
       "        -1.2803559 , -2.7146552 , -2.3774376 , -1.2391273 , -2.1278899 ,\n",
       "        -2.146791  , -2.534984  , -1.7534623 , -1.3251975 , -1.4261572 ,\n",
       "        -2.326664  , -1.4133296 , -1.4294623 , -1.4365782 , -2.2217944 ,\n",
       "        -1.2426871 , -1.246568  ], dtype=float32), 0.00082188845, -4245.78]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the newton optimizer with the current hyper-parameters\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(iteration_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimizer to find hyper-parameters\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(train)\n",
    "sess.run(train)\n",
    "\n",
    "# Initialize hyper-parameters and stopping criterion\n",
    "log_kappa_diff = 1000.0 # This is not a sophisticated stopping criterion!\n",
    "old_log_kappa = sess.run(log_kappa)\n",
    "\n",
    "# Keep training until convergence criterion satisfied\n",
    "while (log_kappa_diff > 0.01):\n",
    "    sess.run(train)\n",
    "    log_kappa_diff = np.abs(sess.run(log_kappa)-old_log_kappa) \n",
    "    old_log_kappa = sess.run(log_kappa)\n",
    "    print(log_kappa_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the log-field at the conditional mode with the empirical Bayes hyper-parameters\n",
    "log_field = sess.run(log_field_at_conditional_mode)\n",
    "slopes = sess.run(slopes)\n",
    "intercept = sess.run(intercept)\n",
    "\n",
    "# Full resolution covariates\n",
    "covariatesfull = pd.read_csv('data/fullcovariates.csv').values[:,1:13]\n",
    "\n",
    "# Evaluate the high-resolution EIR\n",
    "linear_predictor = np.matmul(Afullmatrix,log_field) + np.matmul(covariatesfull,slopes) + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the recovered EIR map\n",
    "\n",
    "# Load in the full resolution covariates tif\n",
    "ds = gdal.Open('data/covariates_AI.tif')\n",
    "covariate = ds.GetRasterBand(1)\n",
    "nodata = covariate.GetNoDataValue()\n",
    "referenceimage = covariate.ReadAsArray()\n",
    "referenceimage = np.ma.masked_less(referenceimage,-3)\n",
    "\n",
    "# Sort out the mask\n",
    "valid_pixels = referenceimage.mask==0\n",
    "valid_pixels_indices = np.nonzero(np.ndarray.flatten(valid_pixels)) # row-major flattening\n",
    "n_valid_pixels = np.sum(valid_pixels)\n",
    "\n",
    "# Get the lats/longs for the pixel values in the rasters\n",
    "x0, dx, dxdy, y0, dydx, dy = ds.GetGeoTransform()\n",
    "nrows, ncols = referenceimage.shape\n",
    "lonarray = np.linspace(x0, x0+dx*ncols, ncols)\n",
    "latarray = np.linspace(y0, y0+dy*nrows, nrows)\n",
    "pixel_longs, pixel_lats = np.meshgrid(lonarray, latarray)\n",
    "\n",
    "# Create image with the inferred EIR\n",
    "referenceimage.put(np.where(np.ndarray.flatten(referenceimage.mask)==0),linear_predictor)\n",
    "\n",
    "# Plot it!\n",
    "plt.scatter(village_longs, village_lats, c='red', s = 1) # check image coordinate transforms by comparing raster plot against observation locations \n",
    "imgplot = plt.imshow(referenceimage,extent=[np.min(pixel_longs),np.max(pixel_longs),np.min(pixel_lats),np.max(pixel_lats)])\n",
    "plt.xlim(np.min(pixel_longs[valid_pixels]),np.max(pixel_longs[valid_pixels]))\n",
    "plt.ylim(np.min(pixel_lats[valid_pixels]),np.max(pixel_lats[valid_pixels]))\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
